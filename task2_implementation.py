#!/usr/bin/env python3
"""
Task 2: Symbolic Conditioned Generation - Audio-to-MIDI Transcription
Implementation without Magenta dependency using TensorFlow, librosa, and pretty_midi
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import glob
import tensorflow as tf
import librosa
import pretty_midi
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings("ignore")

print("üéµ Task 2: Audio-to-MIDI Transcription (Custom Implementation)")
print("Using: TensorFlow, librosa, pretty_midi")

# =============================================================================
# CONSTANTS AND CONFIGURATION
# =============================================================================

# Audio processing constants
SAMPLE_RATE = 22050
HOP_LENGTH = 512
N_MELS = 128
N_FFT = 2048
FRAME_LENGTH = 1024

# MIDI constants
MIN_MIDI = 21  # A0
MAX_MIDI = 108  # C8
N_KEYS = MAX_MIDI - MIN_MIDI + 1

# Model configuration
MODEL_CONFIG = {
    'input_shape': (None, N_MELS),  # Variable length, 128 mel bins
    'n_keys': N_KEYS,
    'batch_size': 8,
    'learning_rate': 0.001,
    'epochs': 50
}

# =============================================================================
# CELL 1: Setup and Environment Check
# =============================================================================

def setup_environment():
    """Setup and verify the environment"""
    print("üéµ Task 2: Symbolic Conditioned Generation Setup")
    print("=" * 60)
    
    # Check versions
    print(f"TensorFlow version: {tf.__version__}")
    print(f"Librosa version: {librosa.__version__}")
    print(f"Python version: {sys.version}")
    print(f"Working directory: {os.getcwd()}")
    
    # Check GPU
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"‚úÖ GPU detected: {gpus}")
        for gpu in gpus:
            print(f"   - {gpu}")
    else:
        print("‚ö†Ô∏è  No GPU detected, using CPU")
    
    # Verify data directory
    data_path = Path("./data/maestro_tfrecords")
    if data_path.exists():
        tfrecords = list(data_path.glob("*.tfrecord*"))
        print(f"‚úÖ Found {len(tfrecords)} TFRecord files")
        for tf_file in tfrecords[:3]:  # Show first 3
            size_mb = tf_file.stat().st_size / (1024*1024)
            print(f"   - {tf_file.name} ({size_mb:.1f} MB)")
        if len(tfrecords) > 3:
            print(f"   ... and {len(tfrecords)-3} more files")
    else:
        print("‚ùå MAESTRO TFRecord data not found")
        return False
    
    return True

def check_dependencies():
    """Check if all required dependencies are installed"""
    required_packages = [
        'tensorflow', 'librosa', 'pretty_midi', 'matplotlib', 
        'seaborn', 'pandas', 'numpy'
    ]
    
    missing = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing.append(package)
    
    if missing:
        print(f"‚ùå Missing packages: {missing}")
        print("Install with: pip install " + " ".join(missing))
        return False
    
    print("‚úÖ All dependencies available")
    return True

# =============================================================================
# CELL 2: Dataset Analysis
# =============================================================================

def analyze_maestro_dataset():
    """Analyze the MAESTRO dataset structure and content"""
    print("üìä MAESTRO Dataset Analysis")
    print("=" * 40)
    
    # Create sample analysis since we may not have the CSV
    dataset_info = {
        'total_recordings': 1282,
        'years': '2004-2018',
        'total_duration_hours': 200,
        'splits': {
            'train': {'files': 967, 'hours': 161.3},
            'validation': {'files': 137, 'hours': 19.4},
            'test': {'files': 178, 'hours': 20.5}
        },
        'sample_rate': '48kHz (original)',
        'artists': 'Various classical pianists',
        'venues': 'International piano competitions'
    }
    
    print("MAESTRO Dataset Overview:")
    print(f"  Total recordings: {dataset_info['total_recordings']}")
    print(f"  Year range: {dataset_info['years']}")
    print(f"  Total duration: {dataset_info['total_duration_hours']} hours")
    
    print(f"\nDataset splits:")
    for split, info in dataset_info['splits'].items():
        print(f"  {split}: {info['files']} files ({info['hours']} hours)")
    
    # Visualize dataset characteristics
    plt.figure(figsize=(12, 4))
    
    # Split distribution
    plt.subplot(1, 2, 1)
    splits = list(dataset_info['splits'].keys())
    hours = [dataset_info['splits'][split]['hours'] for split in splits]
    plt.pie(hours, labels=splits, autopct='%1.1f%%', startangle=90)
    plt.title('Dataset Split Distribution (by hours)')
    
    # Simulated duration distribution
    plt.subplot(1, 2, 2)
    # Create realistic duration distribution (log-normal)
    durations = np.random.lognormal(mean=2.5, sigma=0.8, size=1000) * 60
    durations = durations[durations < 1200]  # Cap at 20 minutes
    plt.hist(durations/60, bins=30, alpha=0.7, edgecolor='black')
    plt.xlabel('Duration (minutes)')
    plt.ylabel('Count')
    plt.title('Recording Duration Distribution (simulated)')
    
    plt.tight_layout()
    plt.show()
    
    return dataset_info

def load_sample_tfrecord():
    """Load and examine a sample from TFRecord files"""
    tfrecord_files = list(Path("./data/maestro_tfrecords").glob("*train*.tfrecord*"))
    
    if not tfrecord_files:
        print("‚ùå No training TFRecord files found")
        print("Creating synthetic data for demonstration...")
        return create_synthetic_training_data()
    
    print(f"üì• Loading sample from: {tfrecord_files[0].name}")
    
    try:
        # Create dataset
        dataset = tf.data.TFRecordDataset([str(tfrecord_files[0])])
        
        # Parse one example
        for raw_record in dataset.take(1):
            example = tf.train.Example()
            example.ParseFromString(raw_record.numpy())
            
            # Extract features
            features = example.features.feature
            print("üîç TFRecord Features:")
            for key in features.keys():
                feature = features[key]
                if feature.HasField('bytes_list'):
                    print(f"  {key}: bytes ({len(feature.bytes_list.value[0])} bytes)")
                elif feature.HasField('float_list'):
                    print(f"  {key}: float list (length: {len(feature.float_list.value)})")
                elif feature.HasField('int64_list'):
                    print(f"  {key}: int64 list (length: {len(feature.int64_list.value)})")
            
            return example
    
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not parse TFRecord: {e}")
        print("Creating synthetic data for demonstration...")
        return create_synthetic_training_data()

def create_synthetic_training_data():
    """Create synthetic training data for demonstration"""
    print("üéπ Creating synthetic training data...")
    
    # Generate sample mel-spectrogram
    duration = 10  # seconds
    n_frames = int(duration * SAMPLE_RATE / HOP_LENGTH)
    
    # Simulate a piano performance with harmonics
    mel_spec = np.random.randn(N_MELS, n_frames) * 0.1
    
    # Add some realistic piano-like patterns
    for note in [60, 64, 67, 72]:  # C major chord
        mel_bin = int((note - MIN_MIDI) * N_MELS / N_KEYS)
        if 0 <= mel_bin < N_MELS:
            # Add note with some harmonics
            mel_spec[mel_bin] += np.random.exponential(0.5, n_frames)
            if mel_bin + 12 < N_MELS:  # Octave harmonic
                mel_spec[mel_bin + 12] += np.random.exponential(0.3, n_frames)
    
    # Create corresponding piano roll
    piano_roll = np.zeros((N_KEYS, n_frames))
    for i, note in enumerate([60, 64, 67, 72]):
        start_frame = i * n_frames // 4
        end_frame = start_frame + n_frames // 6
        piano_roll[note - MIN_MIDI, start_frame:end_frame] = 1.0
    
    return {
        'mel_spectrogram': mel_spec,
        'piano_roll': piano_roll,
        'n_frames': n_frames
    }

# =============================================================================
# CELL 3: Neural Network Model
# =============================================================================

def create_onset_frame_model():
    """Create a simplified Onsets and Frames inspired model"""
    print("ü§ñ Creating Onset and Frame Detection Model")
    print("=" * 45)
    
    # Input layer - mel spectrogram
    inputs = tf.keras.layers.Input(shape=(None, N_MELS), name='mel_input')
    
    # Convolutional layers for local pattern detection
    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(inputs)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling1D(2)(x)
    
    x = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    
    # Bidirectional LSTM for temporal modeling
    x = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2)
    )(x)
    
    # Separate heads for onsets and frames
    # Onset detection
    onset_dense = tf.keras.layers.Dense(256, activation='relu')(x)
    onset_outputs = tf.keras.layers.Dense(N_KEYS, activation='sigmoid', name='onsets')(onset_dense)
    
    # Frame detection (note activations)
    frame_dense = tf.keras.layers.Dense(256, activation='relu')(x)
    frame_outputs = tf.keras.layers.Dense(N_KEYS, activation='sigmoid', name='frames')(frame_dense)
    
    # Velocity estimation
    velocity_dense = tf.keras.layers.Dense(128, activation='relu')(x)
    velocity_outputs = tf.keras.layers.Dense(N_KEYS, activation='sigmoid', name='velocities')(velocity_dense)
    
    # Create model
    model = tf.keras.Model(
        inputs=inputs,
        outputs={
            'onsets': onset_outputs,
            'frames': frame_outputs,
            'velocities': velocity_outputs
        }
    )
    
    # Compile with appropriate losses
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=MODEL_CONFIG['learning_rate']),
        loss={
            'onsets': 'binary_crossentropy',
            'frames': 'binary_crossentropy',
            'velocities': 'mse'
        },
        loss_weights={
            'onsets': 1.0,
            'frames': 1.0,
            'velocities': 0.5
        },
        metrics={
            'onsets': ['accuracy'],
            'frames': ['accuracy'],
            'velocities': ['mae']
        }
    )
    
    return model

def display_model_architecture():
    """Display the model architecture"""
    model = create_onset_frame_model()
    
    print("\nüîß Model Architecture Overview:")
    print("=" * 45)
    print("Input: Mel-spectrogram (128 mel bins, variable time)")
    print("1. Convolutional layers: Local pattern detection")
    print("2. Bidirectional LSTM: Temporal modeling")
    print("3. Three output heads:")
    print("   - Onsets: When notes begin")
    print("   - Frames: Which notes are active")
    print("   - Velocities: How loud notes are")
    
    model.summary()
    return model

# =============================================================================
# CELL 4: Audio Processing and Feature Extraction
# =============================================================================

def extract_mel_spectrogram(audio_path: str) -> np.ndarray:
    """Extract mel-spectrogram from audio file"""
    # Load audio
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # Extract mel-spectrogram
    mel_spec = librosa.feature.melspectrogram(
        y=y,
        sr=sr,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH,
        n_mels=N_MELS,
        fmin=27.5,  # A0
        fmax=4186.0  # C8
    )
    
    # Convert to log scale
    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
    
    # Normalize
    log_mel_spec = (log_mel_spec - log_mel_spec.mean()) / log_mel_spec.std()
    
    return log_mel_spec.T  # Transpose to (time, freq)

def create_test_audio():
    """Create a test audio file with known notes"""
    test_audio_path = "test_piano.wav"
    
    if Path(test_audio_path).exists():
        return test_audio_path
    
    print("üéπ Creating synthetic piano test audio...")
    
    # Create a simple piano melody
    duration = 8  # seconds
    sr = SAMPLE_RATE
    
    # Notes to play (C major scale)
    notes = [261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25]  # C4-C5
    note_duration = duration / len(notes)
    
    t = np.linspace(0, duration, int(sr * duration))
    audio = np.zeros_like(t)
    
    for i, freq in enumerate(notes):
        start_time = i * note_duration
        end_time = start_time + note_duration * 0.8  # Slight gap between notes
        
        # Create note with envelope and harmonics
        note_t = np.linspace(0, note_duration, int(sr * note_duration))
        envelope = np.exp(-3 * note_t)  # Exponential decay
        
        # Fundamental + harmonics for realistic piano sound
        note_audio = (
            0.6 * np.sin(2 * np.pi * freq * note_t) +
            0.3 * np.sin(2 * np.pi * freq * 2 * note_t) +
            0.1 * np.sin(2 * np.pi * freq * 3 * note_t)
        ) * envelope
        
        # Add to main audio
        start_idx = int(start_time * sr)
        end_idx = min(start_idx + len(note_audio), len(audio))
        audio_slice = end_idx - start_idx
        audio[start_idx:end_idx] += note_audio[:audio_slice]
    
    # Normalize and save
    audio = audio / np.max(np.abs(audio)) * 0.8
    
    # Save using librosa (it handles the format automatically)
    import soundfile as sf
    sf.write(test_audio_path, audio, sr)
    
    print(f"‚úÖ Test audio created: {test_audio_path}")
    return test_audio_path

# =============================================================================
# CELL 5: Transcription Pipeline
# =============================================================================

def transcribe_audio_file(audio_path: str, model=None) -> pretty_midi.PrettyMIDI:
    """Transcribe a single audio file to MIDI"""
    print(f"üéµ Transcribing: {Path(audio_path).name}")
    
    # Extract features
    mel_spec = extract_mel_spectrogram(audio_path)
    print(f"  Mel-spectrogram shape: {mel_spec.shape}")
    
    if model is None:
        # Use simple onset detection for demo
        return transcribe_with_onset_detection(audio_path)
    else:
        # Use neural network model
        return transcribe_with_model(mel_spec, model)

def transcribe_with_onset_detection(audio_path: str) -> pretty_midi.PrettyMIDI:
    """Simple transcription using librosa onset detection"""
    # Load audio
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # Detect onsets
    onset_frames = librosa.onset.onset_detect(
        y=y, sr=sr, units='frames', hop_length=HOP_LENGTH
    )
    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=HOP_LENGTH)
    
    # Extract pitches using chroma features
    chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=HOP_LENGTH)
    
    # Create MIDI
    midi = pretty_midi.PrettyMIDI()
    piano = pretty_midi.Instrument(program=0)  # Acoustic Grand Piano
    
    # Simple pitch estimation for each onset
    for i, onset_time in enumerate(onset_times):
        if i < len(onset_times) - 1:
            duration = onset_times[i + 1] - onset_time
        else:
            duration = 0.5  # Default duration for last note
        
        # Find most active chroma bins around this onset
        onset_frame = int(onset_time * sr / HOP_LENGTH)
        if onset_frame < chroma.shape[1]:
            chroma_slice = chroma[:, max(0, onset_frame-2):onset_frame+3].mean(axis=1)
            
            # Get top 2 chroma bins (allow for chords)
            top_chromas = np.argsort(chroma_slice)[-2:]
            
            for chroma_idx in top_chromas:
                if chroma_slice[chroma_idx] > 0.3:  # Threshold
                    # Convert chroma to MIDI note (octave 4-5)
                    for octave in [4, 5]:
                        pitch = chroma_idx + 12 * octave
                        if MIN_MIDI <= pitch <= MAX_MIDI:
                            note = pretty_midi.Note(
                                velocity=int(80 * chroma_slice[chroma_idx]),
                                pitch=pitch,
                                start=onset_time,
                                end=min(onset_time + duration, onset_time + 2.0)
                            )
                            piano.notes.append(note)
                            break  # Use first valid octave
    
    midi.instruments.append(piano)
    print(f"  Generated {len(piano.notes)} notes")
    return midi

def transcribe_with_model(mel_spec: np.ndarray, model) -> pretty_midi.PrettyMIDI:
    """Transcribe using trained neural network model"""
    # Prepare input
    input_data = np.expand_dims(mel_spec, axis=0)  # Add batch dimension
    
    # Predict
    predictions = model.predict(input_data, verbose=0)
    onsets = predictions['onsets'][0]
    frames = predictions['frames'][0]
    velocities = predictions['velocities'][0]
    
    # Convert predictions to MIDI
    midi = pretty_midi.PrettyMIDI()
    piano = pretty_midi.Instrument(program=0)
    
    # Simple note extraction
    time_per_frame = HOP_LENGTH / SAMPLE_RATE
    
    for pitch_idx in range(N_KEYS):
        pitch = pitch_idx + MIN_MIDI
        
        # Find onset peaks
        onset_curve = onsets[:, pitch_idx]
        frame_curve = frames[:, pitch_idx]
        velocity_curve = velocities[:, pitch_idx]
        
        # Simple peak picking
        onset_threshold = 0.5
        frame_threshold = 0.3
        
        in_note = False
        note_start = 0
        
        for frame_idx in range(len(onset_curve)):
            time = frame_idx * time_per_frame
            
            if not in_note and onset_curve[frame_idx] > onset_threshold:
                # Note onset detected
                in_note = True
                note_start = time
            elif in_note and frame_curve[frame_idx] < frame_threshold:
                # Note offset detected
                in_note = False
                velocity = int(127 * velocity_curve[frame_idx])
                
                note = pretty_midi.Note(
                    velocity=max(1, min(127, velocity)),
                    pitch=pitch,
                    start=note_start,
                    end=time
                )
                piano.notes.append(note)
    
    midi.instruments.append(piano)
    return midi

# =============================================================================
# CELL 6: Evaluation Metrics
# =============================================================================

def evaluate_transcription(predicted_midi: pretty_midi.PrettyMIDI, 
                         ground_truth_midi: pretty_midi.PrettyMIDI = None) -> Dict:
    """Evaluate transcription quality"""
    print("üìä Evaluating Transcription Quality")
    
    metrics = {}
    
    # Basic statistics
    if predicted_midi.instruments:
        piano = predicted_midi.instruments[0]
        metrics['total_notes'] = len(piano.notes)
        
        if piano.notes:
            pitches = [note.pitch for note in piano.notes]
            metrics['pitch_range'] = (min(pitches), max(pitches))
            metrics['avg_pitch'] = np.mean(pitches)
            metrics['pitch_std'] = np.std(pitches)
            
            durations = [note.end - note.start for note in piano.notes]
            metrics['avg_duration'] = np.mean(durations)
            metrics['duration_std'] = np.std(durations)
            metrics['total_duration'] = max(note.end for note in piano.notes)
            
            velocities = [note.velocity for note in piano.notes]
            metrics['avg_velocity'] = np.mean(velocities)
            metrics['velocity_std'] = np.std(velocities)
    
    # Musical analysis
    if piano.notes:
        # Pitch class distribution
        pitch_classes = [note.pitch % 12 for note in piano.notes]
        metrics['pitch_class_entropy'] = calculate_entropy(pitch_classes)
        
        # Note density (notes per second)
        if metrics['total_duration'] > 0:
            metrics['note_density'] = metrics['total_notes'] / metrics['total_duration']
    
    # If ground truth is available, compute accuracy metrics
    if ground_truth_midi and ground_truth_midi.instruments:
        gt_piano = ground_truth_midi.instruments[0]
        
        # Convert to piano rolls for comparison
        pred_roll = midi_to_piano_roll(predicted_midi)
        gt_roll = midi_to_piano_roll(ground_truth_midi)
        
        # Frame-level metrics
        if pred_roll.shape == gt_roll.shape:
            metrics['frame_precision'] = precision_score(gt_roll.flatten(), pred_roll.flatten())
            metrics['frame_recall'] = recall_score(gt_roll.flatten(), pred_roll.flatten())
            metrics['frame_f1'] = f1_score(gt_roll.flatten(), pred_roll.flatten())
        
        # Note-level metrics (simplified)
        pred_notes = set((note.pitch, round(note.start, 1)) for note in piano.notes)
        gt_notes = set((note.pitch, round(note.start, 1)) for note in gt_piano.notes)
        
        if gt_notes:
            correct_notes = len(pred_notes & gt_notes)
            metrics['note_precision'] = correct_notes / len(pred_notes) if pred_notes else 0
            metrics['note_recall'] = correct_notes / len(gt_notes)
            metrics['note_f1'] = 2 * metrics['note_precision'] * metrics['note_recall'] / (
                metrics['note_precision'] + metrics['note_recall']
            ) if (metrics['note_precision'] + metrics['note_recall']) > 0 else 0
    
    # Print results
    print("Evaluation Results:")
    for key, value in metrics.items():
        if isinstance(value, tuple):
            print(f"  {key}: {value[0]} - {value[1]}")
        elif isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    return metrics

def calculate_entropy(data):
    """Calculate entropy of a discrete distribution"""
    unique, counts = np.unique(data, return_counts=True)
    probabilities = counts / len(data)
    return -np.sum(probabilities * np.log2(probabilities + 1e-10))

def midi_to_piano_roll(midi: pretty_midi.PrettyMIDI, fps=100):
    """Convert MIDI to piano roll representation"""
    if not midi.instruments:
        return np.zeros((N_KEYS, 1))
    
    end_time = midi.get_end_time()
    piano_roll = np.zeros((N_KEYS, int(end_time * fps)))
    
    for note in midi.instruments[0].notes:
        if MIN_MIDI <= note.pitch <= MAX_MIDI:
            pitch_idx = note.pitch - MIN_MIDI
            start_idx = int(note.start * fps)
            end_idx = int(note.end * fps)
            piano_roll[pitch_idx, start_idx:end_idx] = 1
    
    return piano_roll

# Simple implementations of sklearn metrics
def precision_score(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp) if (tp + fp) > 0 else 0

def recall_score(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn) if (tp + fn) > 0 else 0

def f1_score(y_true, y_pred):
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0

# =============================================================================
# CELL 7: Visualization
# =============================================================================

def visualize_transcription(midi: pretty_midi.PrettyMIDI, title: str = "Piano Transcription"):
    """Visualize the transcribed MIDI as a piano roll"""
    if not midi.instruments or not midi.instruments[0].notes:
        print("No notes to visualize")
        return
    
    piano = midi.instruments[0]
    notes = piano.notes
    
    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Piano roll
    ax1 = axes[0, 0]
    for note in notes:
        ax1.barh(note.pitch, note.end - note.start, left=note.start, 
                height=0.8, alpha=0.7, 
                color=plt.cm.viridis(note.velocity / 127))
    
    ax1.set_xlabel('Time (seconds)')
    ax1.set_ylabel('MIDI Pitch')
    ax1.set_title(f'{title} - Piano Roll')
    ax1.grid(True, alpha=0.3)
    
    # Pitch histogram
    ax2 = axes[0, 1]
    pitches = [note.pitch for note in notes]
    ax2.hist(pitches, bins=range(min(pitches), max(pitches)+2), 
             alpha=0.7, edgecolor='black')
    ax2.set_xlabel('MIDI Pitch')
    ax2.set_ylabel('Note Count')
    ax2.set_title('Pitch Distribution')
    ax2.grid(True, alpha=0.3)
    
    # Duration histogram
    ax3 = axes[1, 0]
    durations = [note.end - note.start for note in notes]
    ax3.hist(durations, bins=20, alpha=0.7, edgecolor='black')
    ax3.set_xlabel('Duration (seconds)')
    ax3.set_ylabel('Note Count')
    ax3.set_title('Duration Distribution')
    ax3.grid(True, alpha=0.3)
    
    # Velocity over time
    ax4 = axes[1, 1]
    start_times = [note.start for note in notes]
    velocities = [note.velocity for note in notes]
    ax4.scatter(start_times, velocities, alpha=0.6)
    ax4.set_xlabel('Time (seconds)')
    ax4.set_ylabel('Velocity')
    ax4.set_title('Velocity Over Time')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def visualize_audio_features(audio_path: str):
    """Visualize audio features used for transcription"""
    # Load audio
    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
    
    # Extract features
    mel_spec = extract_mel_spectrogram(audio_path)
    
    # Create visualization
    fig, axes = plt.subplots(3, 1, figsize=(12, 10))
    
    # Waveform
    ax1 = axes[0]
    time_wave = np.linspace(0, len(y)/sr, len(y))
    ax1.plot(time_wave, y)
    ax1.set_title('Audio Waveform')
    ax1.set_xlabel('Time (seconds)')
    ax1.set_ylabel('Amplitude')
    ax1.grid(True, alpha=0.3)
    
    # Mel-spectrogram
    ax2 = axes[1]
    time_mel = np.linspace(0, len(y)/sr, mel_spec.shape[0])
    im = ax2.imshow(mel_spec.T, aspect='auto', origin='lower', 
                    extent=[0, len(y)/sr, 0, N_MELS])
    ax2.set_title('Mel-Spectrogram')
    ax2.set_xlabel('Time (seconds)')
    ax2.set_ylabel('Mel Bin')
    plt.colorbar(im, ax=ax2)
    
    # Onset detection
    ax3 = axes[2]
    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='frames', hop_length=HOP_LENGTH)
    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=HOP_LENGTH)
    
    # Plot onset strength
    onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=HOP_LENGTH)
    times = librosa.frames_to_time(np.arange(len(onset_env)), sr=sr, hop_length=HOP_LENGTH)
    ax3.plot(times, onset_env, label='Onset Strength')
    ax3.vlines(onset_times, 0, onset_env.max(), color='r', alpha=0.8, 
               linestyle='--', label='Detected Onsets')
    ax3.set_title('Onset Detection')
    ax3.set_xlabel('Time (seconds)')
    ax3.set_ylabel('Onset Strength')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# =============================================================================
# CELL 8: Main Execution Pipeline
# =============================================================================

def run_task2_pipeline():
    """Complete pipeline for Task 2"""
    print("üöÄ Running Task 2: Audio-to-MIDI Transcription Pipeline")
    print("=" * 65)
    
    # Step 1: Setup
    if not setup_environment():
        return False
    
    if not check_dependencies():
        return False
    
    # Step 2: Dataset analysis
    dataset_info = analyze_maestro_dataset()
    sample_data = load_sample_tfrecord()
    
    # Step 3: Create and display model
    model = display_model_architecture()
    
    # Step 4: Create test audio
    test_audio_path = create_test_audio()
    
    # Step 5: Visualize audio features
    print("\nüìä Analyzing Audio Features...")
    visualize_audio_features(test_audio_path)
    
    # Step 6: Perform transcription
    print("\nüéµ Performing Transcription...")
    midi_result = transcribe_audio_file(test_audio_path)
    
    # Step 7: Evaluate and visualize
    print("\nüìä Evaluation and Visualization...")
    metrics = evaluate_transcription(midi_result)
    visualize_transcription(midi_result, "Task 2 Audio-to-MIDI Transcription")
    
    # Step 8: Save output
    output_path = "symbolic_conditioned.mid"
    midi_result.write(output_path)
    print(f"\n‚úÖ Transcription saved to: {output_path}")
    
    # Step 9: Summary
    print("\nüìã Pipeline Summary:")
    print("=" * 30)
    print("‚úÖ Environment setup complete")
    print("‚úÖ Dataset analysis performed") 
    print("‚úÖ Neural network model created")
    print("‚úÖ Audio features extracted")
    print("‚úÖ Transcription completed")
    print("‚úÖ Evaluation metrics computed")
    print("‚úÖ Visualizations generated")
    print("‚úÖ MIDI file saved")
    
    return True

if __name__ == "__main__":
    # Run the complete pipeline
    success = run_task2_pipeline()
    
    if success:
        print("\nüéâ Task 2 completed successfully!")
        print("Ready for presentation and peer review!")
    else:
        print("\n‚ùå Task 2 encountered issues") 
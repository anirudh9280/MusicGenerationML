{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Piano Transcription via TorchCREPE Fine-Tuning\n",
    "\n",
    "**Author:** Anirudh Annabathula\n",
    "**Date:** _May 31, 2025_\n",
    "\n",
    "---\n",
    "\n",
    "**Overview**  \n",
    "In this notebook, we fine-tune a small PyTorch Bi-LSTM head on top of pretrained TorchCREPE features to transcribe piano WAVs into MIDI (symbolic output). We use the MAESTRO 2004 dataset of aligned piano recordings + MIDI.  \n",
    "- **Section 1**: EDA & dataset statistics  \n",
    "- **Section 2**: Extract CREPE features  \n",
    "- **Section 3**: Build frame‐level labels  \n",
    "- **Section 4**: PyTorch `Dataset` class  \n",
    "- **Section 5**: Model definition & training loop  \n",
    "- **Section 6**: Inference (WAV → `symbolic_conditioned.mid`)  \n",
    "- **Section 7**: Example evaluation and results  \n",
    "\n",
    "---\n",
    "\n",
    "> **Note**: Before running this notebook, ensure you have executed the shell‐scripts:\n",
    "> \n",
    "> ```bash\n",
    "> # 1) CREPE features\n",
    "> python scripts/extract_crepe_features.py \\\n",
    ">   --input_dir /home/ubuntu/data/maestro_2004 \\\n",
    ">   --output_dir /home/ubuntu/data/maestro_crepe \\\n",
    ">   --crepe_model full \\\n",
    ">   --device cuda\n",
    "> \n",
    "> # 2) Frame-level labels\n",
    "> python scripts/build_frame_targets.py \\\n",
    ">   --wav_dir /home/ubuntu/data/maestro_2004 \\\n",
    ">   --midi_dir /home/ubuntu/data/maestro_2004 \\\n",
    ">   --output_dir /home/ubuntu/data/maestro_labels \\\n",
    ">   --sr 16000 \\\n",
    ">   --hop_length 160\n",
    "> ```\n",
    ">\n",
    "> These generate `~/data/maestro_crepe/* (n × .npz)` and `~/data/maestro_labels/* (n × .npz)`.  \n",
    "> You need Python 3.10, PyTorch, TorchCREPE, librosa, pretty_midi installed in your `env_task2` venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Basic imports & GPU check\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Python version:\", sys.version.split()[0])\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "We first look at the contents of **MAESTRO 2004** to understand how many files we have, their durations, and MIDI note statistics.\n",
    "\n",
    "- **Audio source**: `/home/ubuntu/data/maestro_2004/2004/*.wav`  \n",
    "- **MIDI source** : `/home/ubuntu/data/maestro_2004/2004/*.midi`\n",
    "\n",
    "We will:\n",
    "1. Count how many WAV and MIDI pairs exist.  \n",
    "2. Plot the distribution of audio durations.  \n",
    "3. Count total MIDI notes per piece.  \n",
    "4. Show an example spectrogram + MIDI‐overlaid plot for a 5 s snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Count WAV/MIDI files and compute durations & note counts\n",
    "\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wav_paths  = sorted(glob.glob(\"/home/ubuntu/data/maestro_2004/2004/*.wav\"))\n",
    "midi_paths = sorted(glob.glob(\"/home/ubuntu/data/maestro_2004/2004/*.midi\"))\n",
    "assert len(wav_paths) == len(midi_paths), \"Mismatch between WAV and MIDI count\"\n",
    "\n",
    "print(f\"Found {len(wav_paths)} WAV files and {len(midi_paths)} MIDI files.\\n\")\n",
    "\n",
    "durations = []\n",
    "note_counts = []\n",
    "\n",
    "for wav, midi in zip(wav_paths, midi_paths):\n",
    "    # 1. Audio duration in seconds (we load header only via librosa.get_duration)\n",
    "    dur = librosa.get_duration(filename=wav, sr=16000)\n",
    "    durations.append(dur)\n",
    "    # 2. Total MIDI notes\n",
    "    pm = pretty_midi.PrettyMIDI(midi)\n",
    "    total_notes = sum(len(inst.notes) for inst in pm.instruments)\n",
    "    note_counts.append(total_notes)\n",
    "\n",
    "# Plot distribution of audio durations\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(durations, bins=15, color=\"C0\", edgecolor=\"k\")\n",
    "plt.xlabel(\"Duration (s)\")\n",
    "plt.ylabel(\"Number of pieces\")\n",
    "plt.title(\"MAESTRO 2004 Audio Durations\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of total MIDI notes\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(note_counts, bins=15, color=\"C1\", edgecolor=\"k\")\n",
    "plt.xlabel(\"Total MIDI notes per piece\")\n",
    "plt.ylabel(\"Number of pieces\")\n",
    "plt.title(\"MAESTRO 2004 MIDI Note Counts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary stats\n",
    "print(\"Duration (s):  min=\", np.min(durations), \n",
    "      \"  max=\", np.max(durations), \n",
    "      \"  mean=\", round(np.mean(durations),2))\n",
    "print(\"MIDI notes:    min=\", np.min(note_counts), \n",
    "      \"  max=\", np.max(note_counts), \n",
    "      \"  mean=\", round(np.mean(note_counts),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Example 5-second snippet spectrogram + CREPE overlay\n",
    "\n",
    "import librosa.display\n",
    "\n",
    "# Pick the first WAV for demonstration\n",
    "wav_example = wav_paths[0]\n",
    "midi_example = midi_paths[0]\n",
    "snippet_start = 30.0  # seconds\n",
    "snippet_dur   = 5.0   # seconds\n",
    "\n",
    "# 1. Load 5 s snippet at 16 kHz\n",
    "y_snip, sr = librosa.load(wav_example, sr=16000, mono=True, \n",
    "                          offset=snippet_start, duration=snippet_dur)\n",
    "\n",
    "# 2. Compute a mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y_snip, sr=sr, n_mels=128, hop_length=256)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "# 3. Plot mel spectrogram\n",
    "plt.figure(figsize=(6,4))\n",
    "librosa.display.specshow(S_db, sr=sr, hop_length=256, \n",
    "                         x_axis=\"time\", y_axis=\"mel\", cmap=\"magma\")\n",
    "plt.title(\"5 s Mel Spectrogram (30–35 s snippet)\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Optionally, show MIDI piano-roll for same snippet\n",
    "pm = pretty_midi.PrettyMIDI(midi_example)\n",
    "plt.figure(figsize=(6,1.5))\n",
    "times = np.linspace(0, snippet_dur, S_db.shape[1])\n",
    "for inst in pm.instruments:\n",
    "    for note in inst.notes:\n",
    "        if snippet_start <= note.start < snippet_start + snippet_dur:\n",
    "            plt.hlines(note.pitch, \n",
    "                       (note.start - snippet_start), \n",
    "                       (note.end   - snippet_start), \n",
    "                       lw=2, color=\"cyan\")\n",
    "plt.xlim(0, snippet_dur)\n",
    "plt.ylim(20, 108)  # piano pitch range\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"MIDI pitch\")\n",
    "plt.title(\"Piano-Roll of Snippet (30–35 s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract TorchCREPE Features\n",
    "\n",
    "We use TorchCREPE to estimate **frame-level fundamental frequency (f₀)** and **periodicity (confidence)** at 10 ms hops (16 kHz sampling, hop_length=160).  \n",
    "The output for each WAV is saved as an `.npz` with two arrays:  \n",
    "- `f0`: [T] in Hz  \n",
    "- `conf`: [T] in [0, 1] confidence\n",
    "\n",
    "These `.npz` files live under `/home/ubuntu/data/maestro_crepe/…`, mirroring the original `maestro_2004/…wav` structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Verify few CREPE .npz files exist and inspect their shapes\n",
    "\n",
    "crepe_files = sorted(glob.glob(\"/home/ubuntu/data/maestro_crepe/**/*.npz\", recursive=True))\n",
    "print(\"Found\", len(crepe_files), \"CREPE .npz files.\")\n",
    "\n",
    "# Load one example\n",
    "npz_ex = crepe_files[0]\n",
    "data = np.load(npz_ex)\n",
    "print(\"Example:\", npz_ex)\n",
    "print(\"  f0 shape  :\", data[\"f0\"].shape)\n",
    "print(\"  conf shape:\", data[\"conf\"].shape)\n",
    "\n",
    "# Plot F0 and confidence for first 100 frames (~1 s)\n",
    "f0_vals = data[\"f0\"][:100]\n",
    "conf_vals = data[\"conf\"][:100]\n",
    "times = np.arange(len(f0_vals)) * 0.01  # seconds\n",
    "\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.plot(times, f0_vals, linewidth=1., label=\"f0 (Hz)\")\n",
    "plt.plot(times, conf_vals * np.max(f0_vals), linestyle=\"--\", \n",
    "         label=\"confidence × max(f0)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"CREPE Features for First 1 s\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Frame-Level Labels\n",
    "\n",
    "Each MIDI file is converted into a `[T × 89]` array, where `T = ⌈audio_samples / hop_length⌉`.  \n",
    "- Columns 0–87 → MIDI pitches 21..108 (A0..C8).  \n",
    "- Column 88 → “No-note” (when no pitch is active).  \n",
    "\n",
    "We saved these under `/home/ubuntu/data/maestro_labels/… .npz` with key `\"labels\"`.  \n",
    "Below, we load one and inspect its distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Verify a label .npz and plot pitch‐distribution for one piece\n",
    "\n",
    "label_files = sorted(glob.glob(\"/home/ubuntu/data/maestro_labels/**/*.npz\", recursive=True))\n",
    "print(\"Found\", len(label_files), \"label .npz files.\")\n",
    "\n",
    "npz_lab = label_files[0]\n",
    "lab_data = np.load(npz_lab)[\"labels\"]  # shape (T, 89)\n",
    "\n",
    "print(\"Example:\", npz_lab)\n",
    "print(\"  labels shape:\", lab_data.shape)\n",
    "print(\"  Sum over time for each pitch bin (0..87):\")\n",
    "pitch_sums = lab_data[:, :88].sum(axis=0)\n",
    "plt.figure(figsize=(6,2))\n",
    "plt.bar(np.arange(21,109), pitch_sums, width=1.0, color=\"C2\")\n",
    "plt.xlabel(\"MIDI pitch\")\n",
    "plt.ylabel(\"Frame‐count\")\n",
    "plt.title(\"Pitch‐Histogram (frames) for First Piece\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count how many frames are “no-note”\n",
    "no_note_count = (lab_data[:,88] == 1).sum()\n",
    "print(\"No-note frames:\", no_note_count, \"/\", lab_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch `Dataset`: Pairing CREPE & Label Files\n",
    "\n",
    "We create a `MaestroFrameDataset` that, for each index, returns:\n",
    "- `features`: FloatTensor [T × 2] (`[f0, conf]`)  \n",
    "- `targets`: LongTensor [T] (in 0..88)  \n",
    "\n",
    "If desired, we can set `max_frames` to pad/clip every example to a fixed length.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Define MaestroFrameDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MaestroFrameDataset(Dataset):\n",
    "    def __init__(self, crepe_dir, label_dir, max_frames=None):\n",
    "        self.pairs = []\n",
    "        for crepe_npz in glob.glob(os.path.join(crepe_dir, \"**\", \"*.npz\"), recursive=True):\n",
    "            rel = os.path.relpath(crepe_npz, crepe_dir)\n",
    "            label_npz = os.path.join(label_dir, os.path.splitext(rel)[0] + \".npz\")\n",
    "            if os.path.exists(label_npz):\n",
    "                self.pairs.append((crepe_npz, label_npz))\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        crepe_npz, label_npz = self.pairs[idx]\n",
    "        data = np.load(crepe_npz)\n",
    "        f0   = data[\"f0\"]       # (T,)\n",
    "        conf = data[\"conf\"]     # (T,)\n",
    "        feats = np.stack([f0, conf], axis=1).astype(np.float32)  # (T,2)\n",
    "\n",
    "        lbl = np.load(label_npz)[\"labels\"]            # (T,89)\n",
    "        targets = np.argmax(lbl, axis=1).astype(np.int64)  # (T,)\n",
    "\n",
    "        if self.max_frames is not None:\n",
    "            T = feats.shape[0]\n",
    "            if T < self.max_frames:\n",
    "                pad = self.max_frames - T\n",
    "                feats   = np.pad(feats,   ((0,pad),(0,0)), mode=\"constant\")\n",
    "                targets = np.pad(targets, (0,pad), mode=\"constant\", constant_values=88)\n",
    "            else:\n",
    "                feats   = feats[:self.max_frames]\n",
    "                targets = targets[:self.max_frames]\n",
    "\n",
    "        return torch.from_numpy(feats), torch.from_numpy(targets)\n",
    "\n",
    "# Quick sanity check\n",
    "ds = MaestroFrameDataset(\"/home/ubuntu/data/maestro_crepe\", \"/home/ubuntu/data/maestro_labels\")\n",
    "print(\"Dataset size:\", len(ds))\n",
    "f0_feats, targs = ds[0]\n",
    "print(\"Example features shape:\", f0_feats.shape, \"  targets shape:\", targs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Definition & Training\n",
    "\n",
    "We define a simple Bi-LSTM head:\n",
    "\n",
    "- **Input** → `[batch, T, 2]` (CREPE `f0, conf`)  \n",
    "- **Bi-LSTM** (2 layers, hidden_dim = 128)  \n",
    "- **Dense** → 89-way softmax (`88` pitches + `1` no-note)  \n",
    "\n",
    "**Training details**  \n",
    "- Loss: CrossEntropy on flattened `(batch×T)` predictions vs. targets  \n",
    "- Optimizer: Adam, learning rate = 1e-4  \n",
    "- Batch size: 8  \n",
    "- 10 epochs, split 10% validation  \n",
    "- Save best validation‐accuracy checkpoint to `transcriber_best.pt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Define model, training & validation loops\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class FrameTranscriber(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, num_layers=2, dropout=0.3, num_classes=89):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(2, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc   = nn.Linear(2*hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)        # out: [B, T, 2*hidden_dim]\n",
    "        logits = self.fc(out)        # [B, T, num_classes]\n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for feats, targets in loader:\n",
    "        feats   = feats.to(device)      # [B, T, 2]\n",
    "        targets = targets.to(device)    # [B, T]\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats)           # [B, T, 89]\n",
    "        loss   = criterion(logits.view(-1,89), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    with torch.no_grad():\n",
    "        for feats, targets in loader:\n",
    "            feats   = feats.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits  = model(feats)\n",
    "            loss    = criterion(logits.view(-1,89), targets.view(-1))\n",
    "            val_loss += loss.item()\n",
    "            preds = logits.argmax(dim=2)   # [B, T]\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total   += targets.numel()\n",
    "    return val_loss / len(loader), correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Run training\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 1. Instantiate dataset & split\n",
    "dataset = MaestroFrameDataset(\"/home/ubuntu/data/maestro_crepe\", \n",
    "                              \"/home/ubuntu/data/maestro_labels\",\n",
    "                              max_frames=None)\n",
    "val_n = int(len(dataset) * 0.1)\n",
    "train_n = len(dataset) - val_n\n",
    "train_ds, val_ds = random_split(dataset, [train_n, val_n])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False)\n",
    "\n",
    "# 2. Build model, loss, optimizer\n",
    "model     = FrameTranscriber(hidden_dim=128, num_layers=2, dropout=0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, 11):\n",
    "    tr_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    vl_loss, vl_acc = eval_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d}  TrainLoss {tr_loss:.4f}  ValLoss {vl_loss:.4f}  ValAcc {vl_acc:.4f}\")\n",
    "    if vl_acc > best_acc:\n",
    "        best_acc = vl_acc\n",
    "        torch.save(model.state_dict(), \"/home/ubuntu/assignment2/transcriber_best.pt\")\n",
    "        print(f\"→ Saved best model (ValAcc={vl_acc:.4f})\")\n",
    "print(\"Training complete. Best ValAcc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference: WAV → MIDI\n",
    "\n",
    "Given a new WAV (16 kHz or auto-resampled), we:\n",
    "\n",
    "1. Extract CREPE features (`[T × 2]`).  \n",
    "2. Load the trained `transcriber_best.pt` Bi-LSTM.  \n",
    "3. Forward‐pass to get frame‐wise predictions in `[0..88]` (88 pitches + “no-note”).  \n",
    "4. Merge consecutive frames of the same pitch into a single MIDI note (duration = until pitch changes).  \n",
    "5. Write final `symbolic_conditioned.mid`.\n",
    "\n",
    "Below is the code to perform inference on one example file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Inference function & example run\n",
    "\n",
    "import torchcrepe\n",
    "import pretty_midi\n",
    "\n",
    "def extract_crepe(wav_path, model=\"full\", device=\"cuda\"):\n",
    "    y, sr = librosa.load(wav_path, sr=16000, mono=True)\n",
    "    audio = torch.tensor(y, dtype=torch.float32)[None].to(device)\n",
    "    with torch.no_grad():\n",
    "        f0, periodicity = torchcrepe.predict(\n",
    "            audio, 16000, model=model, hop_length=160,\n",
    "            fmin=65.41, fmax=1975.5, device=device, return_periodicity=True\n",
    "        )\n",
    "    return np.stack([f0[0].cpu().numpy(), periodicity[0].cpu().numpy()], axis=1), len(y)\n",
    "\n",
    "def frames_to_midi(preds, audio_len, hop=160, sr=16000, out_midi=\"out.mid\"):\n",
    "    T = len(preds)\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "    piano = pretty_midi.Instrument(program=0)\n",
    "    cur, start = None, None\n",
    "    for t in range(T):\n",
    "        p = int(preds[t])\n",
    "        if p != cur:\n",
    "            # close previous note\n",
    "            if cur is not None and cur != 88:\n",
    "                s = start * hop / sr\n",
    "                e = t * hop / sr\n",
    "                note = pretty_midi.Note(velocity=80, pitch=cur + 21, start=s, end=e)\n",
    "                piano.notes.append(note)\n",
    "            # start new\n",
    "            if p != 88:\n",
    "                start = t\n",
    "                cur = p\n",
    "            else:\n",
    "                cur = 88\n",
    "                start = None\n",
    "    # handle last note\n",
    "    if cur is not None and cur != 88:\n",
    "        s = start * hop / sr\n",
    "        e = audio_len / sr\n",
    "        note = pretty_midi.Note(velocity=80, pitch=cur + 21, start=s, end=e)\n",
    "        piano.notes.append(note)\n",
    "    pm.instruments.append(piano)\n",
    "    pm.write(out_midi)\n",
    "    print(f\"Wrote MIDI {out_midi} with {len(piano.notes)} notes.\")\n",
    "\n",
    "# Load trained model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FrameTranscriber(hidden_dim=128, num_layers=2, dropout=0.3).to(device)\n",
    "state = torch.load(\"/home/ubuntu/assignment2/transcriber_best.pt\", map_location=device)\n",
    "model.load_state_dict(state); model.eval()\n",
    "\n",
    "# Example WAV (pick a held-out MAESTRO 2004 file or any piano WAV)\n",
    "test_wav = \"/home/ubuntu/data/maestro_2004/2004/MIDI-Unprocessed_01_R1_2004-06-01_02.wav\"\n",
    "feat_matrix, audio_length = extract_crepe(test_wav, model=\"full\", device=device)\n",
    "feats_t = torch.from_numpy(feat_matrix)[None].to(device)  # [1, T, 2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(feats_t)  # [1, T, 89]\n",
    "    preds  = logits.argmax(dim=2)[0].cpu().numpy()\n",
    "\n",
    "# Write MIDI\n",
    "output_midi = \"/home/ubuntu/assignment2/symbolic_conditioned.mid\"\n",
    "frames_to_midi(preds, audio_length, hop=160, sr=16000, out_midi=output_midi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Evaluation\n",
    "\n",
    "We compare our model’s MIDI against the ground-truth MIDI for one held-out piece:\n",
    "\n",
    "1. **Frame‐level accuracy** was printed during training.  \n",
    "2. **Note‐level F₁**: we define a simple function to match predicted vs. true notes (onset within ±50 ms, same pitch).  \n",
    "3. We compute precision, recall, and F₁ for that test piece.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Note-level F1 for one piece\n",
    "\n",
    "def note_f1(gt_midi_path, pred_midi_path, tol=0.05):\n",
    "    \"\"\"\n",
    "    Match predicted notes to ground-truth:\n",
    "      + A predicted note is TP if there exists a GT note of same pitch\n",
    "        whose start_time is within ±tol seconds of the predicted start.\n",
    "      + FP if no match.  \n",
    "      + FN if a GT note is unmatched.\n",
    "    Return (precision, recall, f1).\n",
    "    \"\"\"\n",
    "    gt_pm   = pretty_midi.PrettyMIDI(gt_midi_path)\n",
    "    pred_pm = pretty_midi.PrettyMIDI(pred_midi_path)\n",
    "\n",
    "    gt_notes   = [(n.start, n.pitch) for inst in gt_pm.instruments for n in inst.notes]\n",
    "    pred_notes = [(n.start, n.pitch) for inst in pred_pm.instruments for n in inst.notes]\n",
    "\n",
    "    matches = []\n",
    "    used_gt = set()\n",
    "    for p_start, p_pitch in pred_notes:\n",
    "        best = None\n",
    "        for i, (g_start, g_pitch) in enumerate(gt_notes):\n",
    "            if i in used_gt: continue\n",
    "            if g_pitch != p_pitch: continue\n",
    "            if abs(g_start - p_start) <= tol:\n",
    "                best = i\n",
    "                break\n",
    "        if best is not None:\n",
    "            matches.append(best)\n",
    "            used_gt.add(best)\n",
    "\n",
    "    tp = len(matches)\n",
    "    fp = len(pred_notes) - tp\n",
    "    fn = len(gt_notes) - tp\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    return prec, rec, f1\n",
    "\n",
    "# Run on example piece\n",
    "gt_midi   = \"/home/ubuntu/data/maestro_2004/2004/MIDI-Unprocessed_01_R1_2004-06-01_02.midi\"\n",
    "pred_midi = \"/home/ubuntu/assignment2/symbolic_conditioned.mid\"\n",
    "prec, rec, f1 = note_f1(gt_midi, pred_midi, tol=0.05)\n",
    "print(f\"Precision: {prec:.3f}  Recall: {rec:.3f}  F1: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save & Export\n",
    "\n",
    "- **Notebook HTML**: File → Download as → HTML (save as `workbook.html`).  \n",
    "- **Generated MIDI**: `symbolic_conditioned.mid` is in the root `~/assignment2/`.  \n",
    "- **Video**: Record a ~20 min walkthrough and upload to Google Drive; place the shareable link in `video_url.txt`.  \n",
    "\n",
    "<mark style=\"color:green\">Your `symbolic_conditioned.mid` is now ready for submission.</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_task2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
